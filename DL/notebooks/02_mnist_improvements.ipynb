{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab485d27",
   "metadata": {},
   "source": [
    "# 02 - Mejoras: Overfitting, Dropout y EarlyStopping (MNIST)\n",
    "\n",
    "En este notebook vamos a mejorar el entrenamiento del modelo:\n",
    "\n",
    "- Veremos qué es **overfitting** (sobreajuste)\n",
    "- Usaremos **Dropout** para regularizar\n",
    "- Usaremos **EarlyStopping** para detener el entrenamiento en el mejor punto\n",
    "- Graficaremos el historial (`history`) para entender qué está pasando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720e9fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d909fb66",
   "metadata": {},
   "source": [
    "## 1) Cargar y preparar datos (MNIST)\n",
    "\n",
    "MNIST ya viene integrado en TensorFlow:\n",
    "- Imágenes: 28x28 en escala de grises\n",
    "- Labels: 0 a 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487cdc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train.shape, y_train.shape\n",
    "\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test  = x_test.astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2fd80a",
   "metadata": {},
   "source": [
    "## 2) ¿Qué es overfitting?\n",
    "\n",
    "**Overfitting (sobreajuste)** sucede cuando:\n",
    "- El modelo se vuelve *muy bueno* en los datos de entrenamiento\n",
    "- Pero empeora (o no mejora) en datos nuevos (validación/test)\n",
    "\n",
    "Se detecta cuando:\n",
    "- `accuracy` (train) sube\n",
    "- pero `val_accuracy` se estanca o baja\n",
    "- y `val_loss` empieza a subir\n",
    "\n",
    "Vamos a entrenar \"más de la cuenta\" para observarlo.\n",
    "\n",
    "## 3) Modelo un poco más grande\n",
    "\n",
    "Aumentaremos la capacidad del modelo (más neuronas), lo cual puede:\n",
    "- mejorar precisión\n",
    "- pero también facilitar overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365ad2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_small = x_train[:10000]\n",
    "y_train_small = y_train[:10000]\n",
    "\n",
    "model_overfit = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_overfit.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model_overfit.summary()\n",
    "\n",
    "history_overfit = model_overfit.fit(\n",
    "    x_train_small, y_train_small,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0644ab8b",
   "metadata": {},
   "source": [
    "## 5) Graficar historial (train vs validation)\n",
    "\n",
    "Vamos a ver:\n",
    "- `loss` vs `val_loss`\n",
    "- `accuracy` vs `val_accuracy`\n",
    "\n",
    "Esto nos ayuda a detectar overfitting visualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60729caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, title=\"Training History\"):\n",
    "    hist = history.history\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(hist[\"loss\"], label=\"loss (train)\")\n",
    "    plt.plot(hist[\"val_loss\"], label=\"loss (val)\")\n",
    "    plt.title(title + \" - Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(hist[\"accuracy\"], label=\"accuracy (train)\")\n",
    "    plt.plot(hist[\"val_accuracy\"], label=\"accuracy (val)\")\n",
    "    plt.title(title + \" - Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history_overfit, \"Model WITHOUT Dropout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d787cc22",
   "metadata": {},
   "source": [
    "## 5) Solución 1: Dropout (regularización)\n",
    "\n",
    "**Dropout** apaga aleatoriamente un % de neuronas durante entrenamiento.\n",
    "\n",
    "¿Por qué ayuda?\n",
    "- Evita que el modelo dependa demasiado de un conjunto pequeño de neuronas\n",
    "- Reduce overfitting\n",
    "- Fuerza al modelo a aprender patrones más generales\n",
    "\n",
    "Nota: Dropout se usa solo en entrenamiento, no en inferencia.\n",
    "\n",
    "## 6) Solución 2: EarlyStopping\n",
    "\n",
    "**EarlyStopping** detiene el entrenamiento si la validación ya no mejora.\n",
    "\n",
    "Ventaja:\n",
    "- Evita entrenar de más\n",
    "- Nos quedamos con el mejor modelo automáticamente\n",
    "\n",
    "Parámetros clave:\n",
    "- `monitor`: qué observar (ej. val_loss)\n",
    "- `patience`: cuántas épocas esperar sin mejora\n",
    "- `restore_best_weights`: regresar al mejor punto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a117c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_regularized = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_regularized.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model_regularized.summary()\n",
    "\n",
    "# EarlyStopping Callbacks\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history_regularized = model_regularized.fit(\n",
    "    x_train_small, y_train_small,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_history(history_regularized, \"Model WITH Dropout + EarlyStopping\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50658f6",
   "metadata": {},
   "source": [
    "## 7) Evaluación final en test\n",
    "\n",
    "Ahora medimos con datos que el modelo nunca vio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118c90ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_dropout.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "print(\"Test loss:\", test_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
